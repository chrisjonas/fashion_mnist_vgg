{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0fd216d4-dd9f-4c8f-9292-df9aa7c17aca",
    "_uuid": "1c70132a25ea771c9caaa6b9dbfd2d65955be21d"
   },
   "source": [
    "**Experimentation on Fashion Mnist with VGG16**\n",
    "To demonstrate\n",
    "1) Converting images with 1 channel to 3 channels\n",
    "2) Resizing the images\n",
    "3) Using VGG16 base model, appending with other layers and extracting features\n",
    "4) Reduce learning, early stopping in callback methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.datasets import fashion_mnist\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16;\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract file names from zip files, convert to dataframe and remove directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipToDf(zip_file):\n",
    "    #collect all file names contained in zip file\n",
    "    with ZipFile(zip_file, 'r') as f:\n",
    "        file_names = f.namelist()\n",
    "    \n",
    "    df = pd.DataFrame({'path':file_names})\n",
    "    \n",
    "    #remove directories\n",
    "    df = df.loc[df.path.str.slice(-1) != '/']\n",
    "    \n",
    "    #remove \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in insta images from #vintagepolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_zip_file = '/Users/chrisjonas/projects/GitHub/insta_images/vintagepolo.zip'\n",
    "\n",
    "insta_df = zipToDf(insta_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24007"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(insta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in eBay images from polo BIN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ebay_zip_file = '/Users/chrisjonas/projects/GitHub/ebay_images/images/polo.zip'\n",
    "\n",
    "ebay_df = zipToDf(ebay_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4808"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ebay_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in eBay photos from polo search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = '/Users/chrisjonas/projects/GitHub/insta_images/vintagepolo.zip'\n",
    "\n",
    "with ZipFile(zip_file, 'r') as f:\n",
    "    file_names = f.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read csv data files\n",
    "train_data = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\n",
    "test_data = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7e89d01e-7dcb-4679-8682-f0901f7acbeb",
    "_uuid": "67003184dbfdac55a3cb8633c25bd38edafd6276",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.shape #(60,000*785)\n",
    "test_data.shape #(10000,785)\n",
    "train_X= np.array(train_data.iloc[:,1:])\n",
    "test_X= np.array(test_data.iloc[:,1:])\n",
    "train_Y= np.array (train_data.iloc[:,0]) # (60000,)\n",
    "test_Y = np.array(test_data.iloc[:,0]) #(10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "108ad2b4-7aa5-4bd1-bdef-f6e3ceed9f31",
    "_uuid": "25e392915c2cdb4b7fef6199c2747db6bfa7ab2d"
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "657bf656-6266-4ff7-9610-a8f4e85b3e29",
    "_uuid": "a736169bc4b6a1d172660b4cc390d98d7c24ac0a"
   },
   "outputs": [],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "90e0b7cd-4941-45e6-9cc5-35966a649a7a",
    "_uuid": "b1878c711627e46e0dc24cc9d1c886d87f2481bd"
   },
   "outputs": [],
   "source": [
    "classes = np.unique(train_Y)\n",
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "65f644bd-0d53-4f93-9d56-44e5ee362124",
    "_uuid": "36b3aa039739b9465903682b38a911cb5550ebd7"
   },
   "outputs": [],
   "source": [
    "# Convert the images into 3 channels\n",
    "train_X=np.dstack([train_X] * 3)\n",
    "test_X=np.dstack([test_X]*3)\n",
    "train_X.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "881b27ff-55a1-47f5-b814-e6e269b0c841",
    "_uuid": "18a5d76e99531a6807c561ee8e4d75f0d726d78f"
   },
   "outputs": [],
   "source": [
    "# Reshape images as per the tensor format required by tensorflow\n",
    "train_X = train_X.reshape(-1, 28,28,3)\n",
    "test_X= test_X.reshape (-1,28,28,3)\n",
    "train_X.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "088dd0e5-b28c-4d44-b7f1-ad972ec93f8b",
    "_uuid": "0fe00caac550a34af0076645c5fa0fa64b2908cc"
   },
   "outputs": [],
   "source": [
    "# Resize the images 48*48 as required by VGG16\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "train_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in train_X])\n",
    "test_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in test_X])\n",
    "#train_x = preprocess_input(x)\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b7f7e12c-01ce-47e5-95bd-069cb89c9258",
    "_uuid": "7bf156c69f540216d0e7b5de3e7b0ad50592099d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalise the data and change data type\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "f2632a1e-c62b-43ca-8417-3dd16176d6a3",
    "_uuid": "6a11e96346d7ff14c5614375ab9237809f4750fd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Labels to one hot encoded format\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "854ef9e9-4fac-4021-8742-258bbc64f7d7",
    "_uuid": "e83f7fb410d318f63141efd5318c4b7ea06c925a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting train data as train and validation data\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X,\n",
    "                                                           train_Y_one_hot,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=13\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "533aef5c-ea10-46f8-8678-ae8466c61358",
    "_uuid": "30c531675abd917e1aff9c7174000a8da61c8e9a"
   },
   "outputs": [],
   "source": [
    "# Finally check the data size whether it is as per tensorflow and VGG16 requirement\n",
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b9a862b8-83a7-4920-9662-4e7d5b06fa2a",
    "_uuid": "113ac9912ec68548ab6c9663d2a268b341d19318",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters for instanitaing VGG16 model. \n",
    "IMG_WIDTH = 48\n",
    "IMG_HEIGHT = 48\n",
    "IMG_DEPTH = 3\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "502df777-d9c5-4a15-9928-c40930a786a7",
    "_uuid": "1efa26a809251bb45f142337617313b8a8b0257c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the input \n",
    "train_X = preprocess_input(train_X)\n",
    "valid_X = preprocess_input(valid_X)\n",
    "test_X  = preprocess_input (test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "229322c0-79c7-499c-b8f5-7eafd8fd45da",
    "_uuid": "3b3f38f3d269f4ee886e7617f9aafe6b2dae6896"
   },
   "outputs": [],
   "source": [
    "#  Create base model of VGG16\n",
    "conv_base = VGG16(weights='../input/keras-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                  include_top=False, \n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
    "                 )\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "aaa03ae6-b327-4155-bb3f-bc276cfc5f79",
    "_uuid": "3303dec8034ae9001f6bbfd172772c3763c0ca95"
   },
   "outputs": [],
   "source": [
    "# Extracting features\n",
    "train_features = conv_base.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "test_features = conv_base.predict(np.array(test_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "val_features = conv_base.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "#for layer in conv_base.layers:\n",
    "#    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "ca1c5a1a-c870-4c5a-a086-77aaa1934f28",
    "_uuid": "edd015a505d1088b0aacc5f7ee5434cfa40929ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.1 Saving the features so that they can be used for future\n",
    "np.savez(\"train_features\", train_features, train_label)\n",
    "np.savez(\"test_features\", test_features, test_Y)\n",
    "np.savez(\"val_features\", val_features, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "06ee60c6-d892-47c9-a29e-498a9a8e6888",
    "_uuid": "afc55e2793e4c9c14330bbe3d2b17c98ff680843"
   },
   "outputs": [],
   "source": [
    "# Current shape of features\n",
    "print(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "da226aa2-e159-4426-9e11-624b109599ae",
    "_uuid": "e8e7f72c0d7a6702c8e68b68ad050d81f052e14d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten extracted features\n",
    "train_features_flat = np.reshape(train_features, (48000, 1*1*512))\n",
    "test_features_flat = np.reshape(test_features, (10000, 1*1*512))\n",
    "val_features_flat = np.reshape(val_features, (12000, 1*1*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "3fae918d3a48e24cbf3de5190f5c0616d13d696c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "f4ca32f8-2a43-453c-838e-efcb89b21df2",
    "_uuid": "4bf35298843795507b757fbcbe2b056f2f51e4b8"
   },
   "outputs": [],
   "source": [
    "# 7.0 Define the densely connected classifier followed by leakyrelu layer and finally dense layer for the number of classes\n",
    "NB_TRAIN_SAMPLES = train_features_flat.shape[0]\n",
    "NB_VALIDATION_SAMPLES = val_features_flat.shape[0]\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
    "model.add(layers.LeakyReLU(alpha=0.1))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "c9c315be-b7c7-484d-9add-0a3aeea4cd14",
    "_uuid": "c22c87623f160988f1aa302bfe76c79c1c08aaba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(),\n",
    "  # optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "77267b8d-0cb3-47df-8aeb-abfc715cdccb",
    "_uuid": "620794510507d763cbdbec85cf88bc74a38c689c"
   },
   "outputs": [],
   "source": [
    "# Incorporating reduced learning and early stopping for callback\n",
    "reduce_learning = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    epsilon=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=0)\n",
    "\n",
    "eary_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=7,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "111c53bc-3f72-426e-abfb-a902d8aa1933",
    "_uuid": "fa421c86f8429c230970142425f236a0e6affb1b"
   },
   "outputs": [],
   "source": [
    "# Train the the model\n",
    "history = model.fit(\n",
    "    train_features_flat,\n",
    "    train_label,\n",
    "    epochs=NB_EPOCHS,\n",
    "    validation_data=(val_features_flat, valid_label),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "098cc9db-404e-4125-9b0a-3b30875cc79a",
    "_uuid": "f9eb383670f32c4c8a4cc5ca3d4b5e8174f924e8"
   },
   "outputs": [],
   "source": [
    "# plot the loss and accuracy\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9cde6c7a-22ef-4c40-a1f9-c23aa3ccbb87",
    "_uuid": "806070772ca7bb192301cdc01dc60bb5b378c59a"
   },
   "source": [
    "References\n",
    "https://www.kaggle.com/crawford/diagnose-lung-disease-with-vgg16\n",
    "https://www.programcreek.com/python/example/92213/keras.applications.vgg16.VGG16\n",
    "http://www.socouldanyone.com/2013/03/converting-grayscale-to-rgb-with-numpy.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
